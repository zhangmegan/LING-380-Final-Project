# -*- coding: utf-8 -*-
"""Japanese-Turkish_FineTuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q3WXY-rEKoQOka8FiyvQfXnqON9m4Sjs

Please note that this code is primarily based on Edan Meyer's following Youtube series on training mT5 models to perform translation tasks: https://www.youtube.com/watch?v=HuZq5KkLx8Q&list=PL_49VD9KwQ_ObGMW5g9hMOLnDY01NHv91.
We have changed the parts of his code relating to hyperparameters, reading in the dataset and dataset manipulation to fit our experiment and data.
"""

!pip install transformers sentencepiece datasets

from datasets import load_dataset
from google.colab import drive
from IPython.display import display
from IPython.html import widgets

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import torch

from torch import optim
from torch.nn import functional as F
from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm_notebook

sns.set()

drive.mount('/content/gdrive')

model_repo = 'google/mt5-small'
model_path = '/content/gdrive/My Drive/mt5_translationNEW3.pt'

tokenizer = AutoTokenizer.from_pretrained(model_repo)

model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)
model = model.cuda()
max_seq_len = model.config.max_length

dataset_en_ja = load_dataset("tatoeba", lang1="en", lang2="ja")
dataset_en_tr = load_dataset("tatoeba", lang1="en", lang2="tr")

#turkish dataset is 672230 lines long
#Japn dataset: 208866 rows

dataset_en_ja2 = dataset_en_ja['train']
dataset_en_tr2 = dataset_en_tr['train']

dataset_ja = dataset_en_ja2.train_test_split(0.532)
dataset_ja = dataset_ja['test']
dataset_ja = dataset_ja.train_test_split(0.1)

train_ja = dataset_ja['train']
test_ja = dataset_ja['test']

train_ja

dataset_tr = dataset_en_tr2.train_test_split(0.0166)
dataset_tr = dataset_tr['test']
dataset_tr = dataset_tr.train_test_split(0.1)


train_tr = dataset_tr['train']
test_tr = dataset_tr['test']

train_tr

LANG_TOKEN_MAPPING = {
    'en': '<en>',
    'ja': '<ja>',
    'tr': '<tr>'
}

special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}
tokenizer.add_special_tokens(special_tokens_dict)
model.resize_token_embeddings(len(tokenizer))

def encode_input_str(text, target_lang, tokenizer, seq_len,
                     lang_token_map=LANG_TOKEN_MAPPING):
  target_lang_token = lang_token_map[target_lang]

  # Tokenize and add special tokens
  input_ids = tokenizer.encode(
      text = target_lang_token + text,
      return_tensors = 'pt',
      padding = 'max_length',
      truncation = True,
      max_length = seq_len)

  return input_ids[0]

def encode_target_str(text, tokenizer, seq_len,
                      lang_token_map=LANG_TOKEN_MAPPING):
  token_ids = tokenizer.encode(
      text = text,
      return_tensors = 'pt',
      padding = 'max_length',
      truncation = True,
      max_length = seq_len)
  
  return token_ids[0]

def format_translation_data(translations, lang_token_map,
                            tokenizer, seq_len=128):

  langs = list(lang_token_map.keys())
  input_lang, target_lang = langs[0], langs[1]

  # Get the translations for the batch
  input_text = translations[input_lang]
  target_text = translations[target_lang]

  if input_text is None or target_text is None:
    return None

  input_token_ids = encode_input_str(
      input_text, target_lang, tokenizer, seq_len, lang_token_map)
  
  target_token_ids = encode_target_str(
      target_text, tokenizer, seq_len, lang_token_map)

  return input_token_ids, target_token_ids

def transform_batch(batch, lang_token_map, tokenizer):
  inputs = []
  targets = []
  for translation_set in batch['translation']:
    formatted_data = format_translation_data(
        translation_set, lang_token_map, tokenizer, max_seq_len)
    
    if formatted_data is None:
      continue
    
    input_ids, target_ids = formatted_data
    inputs.append(input_ids.unsqueeze(0))
    targets.append(target_ids.unsqueeze(0))
    
  batch_input_ids = torch.cat(inputs).cuda()
  batch_target_ids = torch.cat(targets).cuda()

  return batch_input_ids, batch_target_ids

def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):
  dataset = dataset.shuffle()
  for i in range(0, len(dataset), batch_size):
    raw_batch = dataset[i:i+batch_size]
    yield transform_batch(raw_batch, lang_token_map, tokenizer)

n_epochs = 4
batch_size = 16
print_freq = 50
checkpoint_freq = 1000
lr = 5e-4
n_batches = int(np.ceil(len(train_ja) / batch_size))
total_steps = n_epochs * n_batches
n_warmup_steps = int(total_steps * 0.01)

optimizer = AdamW(model.parameters(), lr=lr)
scheduler = get_linear_schedule_with_warmup(optimizer, n_warmup_steps, total_steps)

losses = []

def eval_model(model, gdataset, max_iters=8):
  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,
                                      tokenizer, batch_size)
  eval_losses = []
  for i, (input_batch, label_batch) in enumerate(test_generator):
    if i >= max_iters:
      break

    model_out = model.forward(
        input_ids = input_batch,
        labels = label_batch)
    eval_losses.append(model_out.loss.item())

  return np.mean(eval_losses)

for epoch_idx in range(n_epochs):
   # Randomize data order
   data_generator = get_data_generator(train_ja, LANG_TOKEN_MAPPING, #changed it here
                                       tokenizer, batch_size)
                
   for batch_idx, (input_batch, label_batch) \
       in tqdm_notebook(enumerate(data_generator), total=n_batches):
     optimizer.zero_grad()

     # Forward pass
     model_out = model.forward(
         input_ids = input_batch,
         labels = label_batch)

     # Calculate loss and update weights
     loss = model_out.loss
     losses.append(loss.item())
     loss.backward()
     optimizer.step()
     scheduler.step()

     # Print training update info
     if (batch_idx + 1) % print_freq == 0:
       avg_loss = np.mean(losses[-print_freq:])
       print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(
           epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))
      
     if (batch_idx + 1) % checkpoint_freq == 0:
       test_loss = eval_model(model, test_ja)
       print('Saving model with test loss of {:.3f}'.format(test_loss))
       torch.save(model.state_dict(), model_path)

 torch.save(model.state_dict(), model_path)

"""Now that we fine-tuned with Japanese, we need to fine-tune a little on Turkish.

- Size of each dataset?
- How we decided on how much to fine-tune on?
"""

# Graph the loss

window_size = 50
smoothed_losses = []
for i in range(len(losses)-window_size):
  smoothed_losses.append(np.mean(losses[i:i+window_size]))

plt.plot(smoothed_losses[100:])

LANG_TOKEN_MAPPING = {
    'en': '<en>',
    'tr': '<tr>',
    'ja': '<ja>'
}

n_epochs = 2
batch_size = 16
print_freq = 50
checkpoint_freq = 500
lr = 5e-4
n_batches = int(np.ceil(len(train_tr) / batch_size))
total_steps = n_epochs * n_batches
n_warmup_steps = int(total_steps * 0.01)

optimizer = AdamW(model.parameters(), lr=lr)
scheduler = get_linear_schedule_with_warmup(optimizer, n_warmup_steps, total_steps)

losses = []

for epoch_idx in range(n_epochs):
   # Randomize data order
   data_generator = get_data_generator(train_tr, LANG_TOKEN_MAPPING, #changed it here
                                       tokenizer, batch_size)
                
   for batch_idx, (input_batch, label_batch) \
       in tqdm_notebook(enumerate(data_generator), total=n_batches):
     optimizer.zero_grad()

     # Forward pass
     model_out = model.forward(
         input_ids = input_batch,
         labels = label_batch)

     # Calculate loss and update weights
     loss = model_out.loss
     losses.append(loss.item())
     loss.backward()
     optimizer.step()
     scheduler.step()

     # Print training update info
     if (batch_idx + 1) % print_freq == 0:
       avg_loss = np.mean(losses[-print_freq:])
       print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(
           epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))
      
     if (batch_idx + 1) % checkpoint_freq == 0:
       test_loss = eval_model(model, test_tr)
       print('Saving model with test loss of {:.3f}'.format(test_loss))
       torch.save(model.state_dict(), model_path)

 torch.save(model.state_dict(), model_path)

# Graph the loss

window_size = 50
smoothed_losses = []
for i in range(len(losses)-window_size):
  smoothed_losses.append(np.mean(losses[i:i+window_size]))

plt.plot(smoothed_losses[100:])

"""### Manual Testing

I don't know what I'm doing here

"""



#test_sentence = test_dataset[0]['translation']['en']
test_sentence = "The dog will see the cat."
print('Raw input text:', test_sentence)

input_ids = encode_input_str(
    text = test_sentence,
    target_lang = 'tr',
    tokenizer = tokenizer,
    seq_len = model.config.max_length,
    lang_token_map = LANG_TOKEN_MAPPING)
input_ids = input_ids.unsqueeze(0).cuda()

print('Truncated input text:', tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[0])))

output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=3)
# print(output_tokens)
for token_set in output_tokens:
  print(tokenizer.decode(token_set, skip_special_tokens=True))